\section{Model Regularization via Label Smoothing}
\label{smoothing}
Here we propose a mechanism to regularize the classifier layer by
estimating the marginalized effect of label-dropout during training.

For each training example $x$, our model computes the probability of each label
$k\in\{1\ldots K\}$: $p(k|x) = \frac{\exp(z_k)}{\sum_{i=1}^K \exp(z_i)}$. Here,
$z_i$ are the {\em logits} or unnormalized log-probabilities. Consider the
ground-truth distribution over labels $q(k|x)$ for this training example,
normalized so that $\sum_k q(k|x) = 1$. For brevity, let us omit the dependence
of $p$ and $q$ on example $x$. We define the loss for the example as the
cross entropy: $\ell = -\sum_{k=1}^K \log(p(k)) q(k)$. Minimizing this is
equivalent to maximizing the expected log-likelihood of a label, where the label
is selected according to its ground-truth distribution $q(k)$. Cross-entropy
loss is differentiable with respect to the logits $z_k$ and thus can be used for
gradient training of deep models. The gradient has a rather simple form:
$\frac{\partial\ell}{\partial z_k} = p(k) - q(k)$, which is bounded between $-1$
and $1$.

Consider the case of a single ground-truth label $y$, so that $q(y)=1$ and
$q(k)=0$ for all $k\neq y$. In this case, minimizing the cross entropy is
equivalent to maximizing the log-likelihood of the correct label. For a
particular example $x$ with label $y$, the log-likelihood is maximized for $q(k)
= \delta_{k,y}$, where $\delta_{k,y}$ is Dirac delta, which equals $1$ for $k=y$
and $0$ otherwise. This maximum is not achievable for finite $z_k$ but is
approached if $z_y\gg z_k$ for all $k\neq y$ -- that is, if the logit
corresponding to the ground-truth label is much great than all other
logits. This, however, can cause two problems. First, it may result in
over-fitting: if the model learns to assign full probability to the ground-truth
label for each training example, it is not guaranteed to generalize. Second, it
encourages the differences between the largest logit and all others to become
large, and this, combined with the bounded gradient
$\frac{\partial\ell}{\partial z_k}$, reduces the ability of the model to
adapt. Intuitively, this happens because the model becomes too confident about
its predictions.

We propose a mechanism for encouraging the model to be less confident. While
this may not be desired if the goal is to maximize the log-likelihood of
training labels, it does regularize the model and makes it more adaptable. The
method is very simple. Consider a distribution over labels $u(k)$, {\em
  independent of the training example $x$}, and a smoothing parameter $\epsilon$.
 For a training example with ground-truth label $y$, we
replace the label distribution $q(k|x)=\delta_{k,y}$ with
$$
q'(k|x) = (1-\epsilon) \delta_{k,y} + \epsilon u(k)
$$
which is a mixture of the original ground-truth distribution $q(k|x)$ and the
fixed distribution $u(k)$, with weights $1-\epsilon$ and $\epsilon$,
respectively. This can be seen as the distribution of the label $k$ obtained as
follows: first, set it to the ground-truth label $k=y$; then, with probability
$\epsilon$, replace $k$ with a sample drawn from the distribution $u(k)$. We
propose to use the prior distribution over labels as $u(k)$. In our experiments,
we used the uniform distribution $u(k) = 1/K$, so that
$$
q'(k) = (1-\epsilon) \delta_{k,y} + \frac{\epsilon}{K}.
$$
We refer to this change in ground-truth label distribution as {\em
  label-smoothing regularization}, or LSR.

Note that LSR achieves the desired goal of preventing the largest logit from
becoming much larger than all others. Indeed, if this were to happen, then a
single $q(k)$ would approach $1$ while all others would approach $0$. This would
result in a large cross-entropy with $q'(k)$ because, unlike
$q(k)=\delta_{k,y}$, all $q'(k)$ have a positive lower bound.

Another interpretation of LSR can be obtained by considering the cross entropy:
$$
H(q',p) = -\sum_{k=1}^K \log p(k) q'(k) = (1-\epsilon)H(q, p) + \epsilon H(u, p)
$$
Thus, LSR is equivalent to replacing a single cross-entropy loss $H(q,p)$ with a
pair of such losses $H(q,p)$ and $H(u,p)$. The second loss penalizes the
deviation of predicted label distribution $p$ from the prior $u$, with the 
relative weight $\frac{\epsilon}{1-\epsilon}$. Note that this deviation could be
equivalently captured by the KL divergence, since $H(u,p) = D_{KL}(u\|p) + H(u)$
and $H(u)$ is fixed. When $u$ is the uniform distribution, $H(u,p)$ is a measure
of how dissimilar the predicted distribution $p$ is to uniform, which could also be
measured (but not equivalently) by negative entropy $-H(p)$; we have not
experimented with this approach.

In our ImageNet experiments with $K=1000$ classes, we used $u(k) = 1/1000$ and
$\epsilon=0.1$. For ILSVRC 2012, we have found a consistent improvement of
about $0.2\%$ absolute both for top-$1$ error and the top-$5$ error
(cf. Table~\ref{results}).
